{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "99d9b702a3fdc1261165d68d8b27b9b4686d1bea"
   },
   "source": [
    "### Relevant imports/variables.\n",
    "These are mostly straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f4476aa232321010519c726c0bc57c3597c70b5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af25616b56a2f6c2f8829b3a2633fb080320b64a"
   },
   "outputs": [],
   "source": [
    "# Some global constants.\n",
    "NUM_PREDICTOR_COLS = 784\n",
    "PREDICTOR_COLS = ['pixel' + str(i) for i in range(NUM_PREDICTOR_COLS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f88bd988adadc77e96b4719ca2bb2b6254d05d27"
   },
   "source": [
    "### Relevant helper routines\n",
    "These are self explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e275122c32d97be9fee8a9c6016e434369b5c29"
   },
   "outputs": [],
   "source": [
    "def create_pipeline_and_out_of_sample_score(given_model, \n",
    "                                            X_train, \n",
    "                                            X_test, \n",
    "                                            Y_train, \n",
    "                                            Y_test):\n",
    "    # Rewrite everything as a pipeline\n",
    "    my_pipeline = make_pipeline(given_model)\n",
    "    my_pipeline.fit(X_train, Y_train.values.ravel())\n",
    "    predictions = my_pipeline.predict(X_test)\n",
    "    out_of_sample_score = accuracy_score(predictions, Y_test)\n",
    "    return (my_pipeline, out_of_sample_score, predictions)\n",
    "\n",
    "def cross_validate(my_pipeline, X, Y):\n",
    "    cross_val_scores = \\\n",
    "        cross_val_score(my_pipeline, X, Y, scoring='accuracy', cv=5)\n",
    "\n",
    "    return cross_val_scores.mean()\n",
    "\n",
    "def train_test_cross_validate(train_data,\n",
    "                              given_model,\n",
    "                              do_cross_validation=True,\n",
    "                              X_columns=PREDICTOR_COLS, \n",
    "                              Y_columns=['label']):\n",
    "    (X_train, X_test, Y_train, Y_test, X, Y) = \\\n",
    "        get_train_test_data(train_data, X_columns, Y_columns)\n",
    "    my_pipeline, out_of_sample_score, predictions_test = \\\n",
    "        create_pipeline_and_out_of_sample_score(given_model, X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "    predictions_train = my_pipeline.predict(X_train)\n",
    "    num_correct_predictions_train = int((accuracy_score(predictions_train, Y_train)) * len(Y_train))\n",
    "    num_correct_predictions_train1 = np.sum(predictions_train == Y_train.values.ravel())\n",
    "    print('Training score is {0}'.format((accuracy_score(predictions_train, Y_train)) ))\n",
    "    if do_cross_validation:\n",
    "        cross_validation_score = cross_validate(make_pipeline(given_model), X, Y.values.ravel())\n",
    "    else:\n",
    "        cross_validation_score = -1\n",
    "\n",
    "    return (my_pipeline, out_of_sample_score, cross_validation_score)\n",
    "\n",
    "def get_train_test_data(train_data,\n",
    "                        X_columns=PREDICTOR_COLS, \n",
    "                        Y_columns=['label']):\n",
    "    # Simple training and testing\n",
    "    X = train_data[X_columns]\n",
    "    Y = train_data[Y_columns]\n",
    " \n",
    "    # Do imputation on relevant columns.\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)\n",
    "    return (X_train, X_test, Y_train, Y_test, X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69d5ed4c807ed45fe8687aa68151acfc5b99e6a3"
   },
   "source": [
    "### Reading in data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac442685b751b781ef61afa91a9f126c83c8c817"
   },
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed03b0bf3eae70bb591f215578030b18f31b5d35"
   },
   "source": [
    "### Checking for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fda666b14209543d19c136591272e6b29a13f1ef"
   },
   "outputs": [],
   "source": [
    "full_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a68c0415c18ec7146eab063a48533266b5ecfe7b"
   },
   "source": [
    "### Split into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29f1a9e23ddc54f250e45604b4ddafdfaf529064"
   },
   "outputs": [],
   "source": [
    "LEN_TRAIN_SET = int(0.8 * len(full_data))\n",
    "train_data = full_data[0:LEN_TRAIN_SET]\n",
    "validation_data = full_data[LEN_TRAIN_SET:len(full_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f60c96701df114d2adcec8c4e3be1ac5058b73e"
   },
   "outputs": [],
   "source": [
    "assert(len(train_data) + len(validation_data) == len(full_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e6e3defba4bca52fbc49230d55d9b9153a2afd2"
   },
   "source": [
    "### Set up for doing cross validation\n",
    "Later , we try several versions playing with the n_estimators parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c92742dae1e079293aa9bba6e20c3b9ef7a73497"
   },
   "outputs": [],
   "source": [
    "# Commenting out, as we have another implemention later.\n",
    "#(my_pipeline, out_of_sample_score, cross_validation_score) = \\\n",
    "#    train_test_cross_validate(full_data,\n",
    "#                              XGBClassifier(seed=1),\n",
    "#                              do_cross_validation=False,\n",
    "#                              X_columns=PREDICTOR_COLS, \n",
    "#                              Y_columns=['label'])    \n",
    "#print(\"Out of sample score is {0}\\nCross val score is {1}\".format(out_of_sample_score, cross_validation_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8745ed049bfbd4e5a78c018616cfcc390c79caef"
   },
   "source": [
    "#### Using xgboost exact mode.\n",
    "Till now, we we have used the sklearn wrapper for xgboost. However, it is recommended to use  the exact mode as that gives more flexibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27b4b3a2cac7d938dbfc28861bb27e9a987fc3c0"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "train_label = train_data[['label']]\n",
    "validation_label = validation_data[['label']]\n",
    "dtrain = xgb.DMatrix(train_data[PREDICTOR_COLS], label=train_label)\n",
    "dvalid = xgb.DMatrix(validation_data[PREDICTOR_COLS], label=validation_label)\n",
    "\n",
    "watchlist = [(dvalid, 'dvalidation'), (dtrain, 'training')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section on cross validation/out of sample testing.\n",
    "\n",
    "This has been commented out to reduce the running time of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b767153dc66e4ef09c1c445144071ae170969044"
   },
   "outputs": [],
   "source": [
    "#eta_num_rounds = [(0.3, 50), (0.2, 100), (0.1, 150), [0.05, 200], [0.02, 300]]\n",
    "\n",
    "\n",
    "#for elem in eta_num_rounds :\n",
    "#    eta = elem[0]\n",
    "#    num_boost_rounds = elem[1]\n",
    "#    param = {'eta' : eta, 'objective' : 'multi:softmax', 'num_class' : 10}\n",
    "#    res = xgb.cv(param, dtrain, num_boost_round=num_boost_rounds, metrics={'merror'}, early_stopping_rounds=5 )\n",
    "#    print(res[['train-merror-mean', 'train-merror-std']])\n",
    "#    print(res[['test-merror-mean', 'test-merror-std']])\n",
    " #   print(eta, num_boost_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa2dfc7c75b0fb5928f1d9799a00dca185b2eb57"
   },
   "outputs": [],
   "source": [
    "#for elem in eta_num_rounds :\n",
    "#    eta = elem[0]\n",
    "#    num_boost_rounds = elem[1]\n",
    "#    param = {'eta' : eta, 'objective' : 'multi:softmax', 'num_class' : 10}\n",
    "#    bst = xgb.train(param, dtrain, num_boost_round=num_boost_rounds, evals=watchlist, early_stopping_rounds=5 )\n",
    "#    print(eta, num_boost_rounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0a87d53bacc105bce88a8823d4f3a077b149f7e"
   },
   "outputs": [],
   "source": [
    "eta = 0.2\n",
    "num_boost_rounds = 100\n",
    "param = {'eta' : eta, 'objective' : 'multi:softmax', 'num_class' : 10}\n",
    "bst = xgb.train(param, dtrain, num_boost_round=num_boost_rounds, evals=watchlist, early_stopping_rounds=5 )\n",
    "print(eta, num_boost_rounds)\n",
    "\n",
    "predictions_valid = bst.predict(dvalid)\n",
    "(predictions_valid != validation_data['label'].values).sum()/len(predictions_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df3f46e1d6a1a86dce687f21dff7d81231c0335a"
   },
   "source": [
    "### Making predictions on  test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64678f591d152a60bb496d5f1cc9f31637a33e99"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../input/test.csv')\n",
    "test_predictions = my_pipeline.predict(test_data[PREDICTOR_COLS])\n",
    "test_data['label'] = test_predictions\n",
    "test_data['ImageId'] = test_data.index + 1\n",
    "test_data[['ImageId', 'label']].to_csv('submission_boosting_trees_xgboost.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
